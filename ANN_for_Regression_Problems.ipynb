{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTkcuw99gHEB"
   },
   "source": [
    "# ANN for Regression Problems\n",
    "In this example we will be seeing how to model an ANN for a regression problem. We will be using a car prices data set. \n",
    "The data set contains the following parameters:\n",
    "* Price: The Price of the car in dollars\n",
    "* Age: The age of the car in months\n",
    "* KM: How many KMS did the car was used\n",
    "* HP: Horsepower of the car\n",
    "* MetColor: Whether the car has a metallic color or not\n",
    "* CC: The engine size of the car\n",
    "* Doors: The number of doors in the car\n",
    "* Weight: The weight of the car\n",
    "\n",
    "Here the target variable is Price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "T9cBty0Bh_dr"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SXNqm5mfjz0n",
    "outputId": "c3ad9ce4-ecde-4738-e5e2-fc28b6dcaa80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-05e1d43e-ee49-4f13-8a06-ddc61a17d413\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>KM</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HP</th>\n",
       "      <th>MetColor</th>\n",
       "      <th>CC</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>46986</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>72937</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41711</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38500</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05e1d43e-ee49-4f13-8a06-ddc61a17d413')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-05e1d43e-ee49-4f13-8a06-ddc61a17d413 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-05e1d43e-ee49-4f13-8a06-ddc61a17d413');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Unnamed: 0   Age     KM  Weight  HP  MetColor      CC  Doors  Price\n",
       "0           0  23.0  46986  1165.0  90         1  2000.0      3  13500\n",
       "1           1  23.0  72937  1165.0  90         1  2000.0      3  13750\n",
       "2           2  24.0  41711  1165.0  90         1  2000.0      3  13950\n",
       "3           3  26.0  48000  1165.0  90         0  2000.0      3  14950\n",
       "4           4  30.0  38500  1170.0  90         0  2000.0      3  13750"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the pickle file \n",
    "cardata=pd.read_csv('CarPricesData.csv')\n",
    "cardata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoyehmCFkC3H"
   },
   "source": [
    "Now that the data is ready let us define our target and predictor variables and scale them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "C1eqYCKAj7k6"
   },
   "outputs": [],
   "source": [
    "# Separating the Target Variable and Predictor Variables\n",
    "X = cardata[['Age', 'KM',\t'Weight',\t'HP',\t'MetColor',\t'CC',\t'Doors']].values\n",
    "y = cardata['Price'].values\n",
    "\n",
    "# To further fit and transform our variable we must reshape it to a 2 dimensional array\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "VfGnrpfZk_V8"
   },
   "outputs": [],
   "source": [
    "# Scaling the data using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "XScaler=StandardScaler()\n",
    "yScaler=StandardScaler()\n",
    " \n",
    "# Storing the fit object for later reference\n",
    "XScalerFit = XScaler.fit(X)\n",
    "yScalerFit = yScaler.fit(y)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "X=XScalerFit.transform(X)\n",
    "y=yScalerFit.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "v1tgtDiUwPzx"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETa5_GB_nEGB"
   },
   "source": [
    "Now that our data has been scaled and split, let us begin with modeling the data. \n",
    "\n",
    "The \"Sequential\" module from the Keras package is used in the following code snippet to generate a succession of ANN layers stacked one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "-nUNq12Zm4s4"
   },
   "outputs": [],
   "source": [
    "# Initializing Artificial Neural Networks\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEgbX3WcoZ-r"
   },
   "source": [
    "For our model, we will be using two hidden layers with five neurons each and one output layer with one neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "CnkvUHrunqCZ"
   },
   "outputs": [],
   "source": [
    "# Defining the first hidden layer which is also the input layer\n",
    "ann.add(tf.keras.layers.Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqOB1GHqqX1a"
   },
   "source": [
    "Here, \n",
    "* units=5 means we are creating a layer with five neurons in it. Each of these five neurons will be receiving the values of inputs.\n",
    "* input_dim=7 means there are seven predictors in the input data which is expected by the first layer. If you see the second dense layer, we don’t specify this value, because the Sequential model passes this information further to the next layers.\n",
    "* kernel_initializer=’normal’ When the Neurons start their computation, some algorithm has to decide the value for each weight. This parameter specifies that. You can choose different values for it like ‘normal’ or ‘glorot_uniform’.\n",
    "* activation=’relu’ specifies the activation function for the calculations inside each neuron. You can choose values like ‘relu’, ‘tanh’, ‘sigmoid’, etc.\n",
    "* batch_size=20 specifies how many rows will be passed to the Network in one go after which the SSE calculation will begin and the neural network will start adjusting its weights based on the errors.\n",
    "* Epochs=50 The same activity of adjusting weights continues for 50 times, as specified by this parameter. In simple terms, the ANN looks at the full training data 50 times and adjusts its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "NfpTHC6QqXEu"
   },
   "outputs": [],
   "source": [
    "# Defining the second hidden layer of the model\n",
    "# Here, we don't have to specify input_dim as keras configure it automatically\n",
    "ann.add(tf.keras.layers.Dense(units=5, kernel_initializer='normal', activation='relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "dKuOw5nYrhos"
   },
   "outputs": [],
   "source": [
    "# The output neuron is a single fully connected node since we will be predicting a single number\n",
    "ann.add(tf.keras.layers.Dense(units=1,kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "DwRjT5Cyr5lZ"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "ann.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-uJNq6ZKr9vU",
    "outputId": "19215a22-e374-4ccb-ba37-cda69bb635c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "58/58 [==============================] - 1s 2ms/step - loss: 1.0166\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.9475\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.6807\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3398\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1869\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1525\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1395\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1325\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1285\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1259\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1239\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1220\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1205\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1195\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1188\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1170\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1153\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1154\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1142\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1136\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1124\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1113\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1088\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1066\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1064\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1041\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1033\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1025\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0996\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0999\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0987\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0995\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0988\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0989\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0983\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0984\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0980\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0979\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0983\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0983\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0978\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0972\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0977\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0969\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0973\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0973\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0972\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0968\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0962\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0959\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0970\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0962\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0963\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.0961\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0958\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0953\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0948\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0948\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0951\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0946\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0950\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0951\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0944\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0941\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0944\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0943\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0938\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0938\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0944\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0948\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0940\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0931\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0932\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0937\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0934\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0932\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.0927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01ee433710>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "ann.fit(X_train, y_train ,batch_size = 20, epochs = 100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aou2itKoxBK6"
   },
   "source": [
    "Here, we see that our model has performed quite well in reducing the mean squared error which is our loss function. Further we can try to loop through a set of epochs and batch size values to alter the performance to suit us best. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ANN for Regression Problems.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
